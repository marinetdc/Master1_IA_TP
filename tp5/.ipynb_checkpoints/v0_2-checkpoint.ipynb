{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Déséquilibre en classification et Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Classification à partir d'un jeu de données déséqilibré "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Jeu de données artificiellement généré "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Rapport avec déséquilibre ************\n"
     ]
    }
   ],
   "source": [
    "X,Y = make_classification(n_samples=150, n_features=20, n_informative=10,\n",
    "                                n_redundant=5, n_repeated=5, n_classes=2,\n",
    "                                n_clusters_per_class=2, weights=[0.9, 0.1],\n",
    "                                flip_y=0.01, class_sep=1.5, hypercube=True)\n",
    "cnames=[\"M\",\"m\"]\n",
    "\n",
    "X_app,X_test,Y_app,Y_test=train_test_split(X,Y,test_size=0.30,random_state=12)\n",
    "\n",
    "print(\"*********** Rapport avec déséquilibre ************\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maj:               precision    recall  f1-score   support\n",
      "\n",
      "           M       0.89      0.83      0.86        41\n",
      "           m       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.76        45\n",
      "   macro avg       0.45      0.41      0.43        45\n",
      "weighted avg       0.82      0.76      0.78        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_nb = DummyClassifier(strategy=\"stratified\") \n",
    "clf_nb.fit(X_app,Y_app)\n",
    "y_pred_nb = clf_nb.predict(X_test)\n",
    "print(\"Maj:\",classification_report(Y_test, y_pred_nb, target_names=cnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB:                precision    recall  f1-score   support\n",
      "\n",
      "           M       0.98      1.00      0.99        41\n",
      "           m       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.99      0.88      0.92        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_nb = GaussianNB()\n",
    "clf_nb.fit(X_app,Y_app)\n",
    "y_pred_nb = clf_nb.predict(X_test)\n",
    "print(\"NB: \",classification_report(Y_test, y_pred_nb, target_names=cnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT:                precision    recall  f1-score   support\n",
      "\n",
      "           M       0.98      1.00      0.99        41\n",
      "           m       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.99      0.88      0.92        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_dt.fit(X_app,Y_app)\n",
    "y_pred_nb = clf_dt.predict(X_test)\n",
    "print(\"DT: \",classification_report(Y_test, y_pred_nb, target_names=cnames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KP:                precision    recall  f1-score   support\n",
      "\n",
      "           M       0.91      1.00      0.95        41\n",
      "           m       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.46      0.50      0.48        45\n",
      "weighted avg       0.83      0.91      0.87        45\n",
      "\n",
      "C:\\Users\\Artau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Artau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Artau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "clf_kppv = KNeighborsClassifier()\n",
    "clf_kppv.fit(X_app,Y_app)\n",
    "y_pred_nb = clf_kppv.predict(X_test)\n",
    "print(\"KP: \",classification_report(Y_test, y_pred_nb, target_names=cnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessus renvoie le rapport de classification de plusieurs classifieur sur un jeu de données déséquililbré. \n",
    "\n",
    "Le rapport renvoit comme information : \n",
    "\n",
    "- Precision : le rapport entre les vrais positifs et la somme des vrais et faux positifs. C'est l'exactitude du classifieur pour une classe. \n",
    "    - *Accuracy of positive predictions.*\n",
    "    - *TP/(TP + FP)*\n",
    "- Recall : mesure de la complétude du classifieur, c'est-à-dire la capacité d'un classifieur à trouver correctement toutes les instances positives.\n",
    "    - *Fraction of positives that were correctly identified.*\n",
    "    - *TP/(TP+FN)*\n",
    "- f1-score : moyenne harmonique pondérée de la précision et du rappel\n",
    "    - *En règle générale, la moyenne pondérée de F1 devrait être utilisée pour comparer les modèles de classificateurs, et non la précision globale.*\n",
    "    - *2x(Recall x Precision) / (Recall + Precision)*\n",
    "- Support : le nombre d'occurrences réelles de la classe dans l'ensemble de données spécifié\n",
    "\n",
    "\n",
    "- macro avg : moyenne non pondérée \n",
    "- weighted avg : moyenne pondérée par le support (le nombre d'instances vraies pour chaque étiquette). Cela modifie \" macro \" pour prendre en compte le déséquilibre des étiquettes ; cela peut donner un score F qui ne se situe pas entre la précision et le rappel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Reference 1](https://www.scikit-yb.org/en/latest/api/classifier/classification_report.html)\n",
    "[Reference 2](https://muthu.co/understanding-the-classification-report-in-sklearn/)\n",
    "[Reference 3](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Rapport avec déséquilibre / weights = [0.1, 0.9]************\n",
      "Maj:               precision    recall  f1-score   support\n",
      "\n",
      "           M       0.86      0.80      0.83        40\n",
      "           m       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.71        45\n",
      "   macro avg       0.43      0.40      0.42        45\n",
      "weighted avg       0.77      0.71      0.74        45\n",
      "\n",
      "NB:                precision    recall  f1-score   support\n",
      "\n",
      "           M       0.97      0.93      0.95        40\n",
      "           m       0.57      0.80      0.67         5\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.77      0.86      0.81        45\n",
      "weighted avg       0.93      0.91      0.92        45\n",
      "\n",
      "DT:                precision    recall  f1-score   support\n",
      "\n",
      "           M       0.97      0.95      0.96        40\n",
      "           m       0.67      0.80      0.73         5\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.82      0.88      0.84        45\n",
      "weighted avg       0.94      0.93      0.94        45\n",
      "\n",
      "KP:                precision    recall  f1-score   support\n",
      "\n",
      "           M       0.95      1.00      0.98        40\n",
      "           m       1.00      0.60      0.75         5\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.98      0.80      0.86        45\n",
      "weighted avg       0.96      0.96      0.95        45\n",
      "\n",
      "*********** Rapport avec déséquilibre / weights = [0.2, 0.8]************\n",
      "Maj:               precision    recall  f1-score   support\n",
      "\n",
      "           M       0.98      0.93      0.95        44\n",
      "           m       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.49      0.47      0.48        45\n",
      "weighted avg       0.95      0.91      0.93        45\n",
      "\n",
      "NB:                precision    recall  f1-score   support\n",
      "\n",
      "           M       1.00      0.91      0.95        44\n",
      "           m       0.20      1.00      0.33         1\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.60      0.95      0.64        45\n",
      "weighted avg       0.98      0.91      0.94        45\n",
      "\n",
      "DT:                precision    recall  f1-score   support\n",
      "\n",
      "           M       1.00      0.91      0.95        44\n",
      "           m       0.20      1.00      0.33         1\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.60      0.95      0.64        45\n",
      "weighted avg       0.98      0.91      0.94        45\n",
      "\n",
      "KP:                precision    recall  f1-score   support\n",
      "\n",
      "           M       1.00      0.95      0.98        44\n",
      "           m       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.67      0.98      0.74        45\n",
      "weighted avg       0.99      0.96      0.97        45\n",
      "\n",
      "*********** Rapport avec déséquilibre / weights = [0.3, 0.7]************\n",
      "Maj:               precision    recall  f1-score   support\n",
      "\n",
      "           M       0.90      0.88      0.89        41\n",
      "           m       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.80        45\n",
      "   macro avg       0.45      0.44      0.44        45\n",
      "weighted avg       0.82      0.80      0.81        45\n",
      "\n",
      "NB:                precision    recall  f1-score   support\n",
      "\n",
      "           M       0.97      0.93      0.95        41\n",
      "           m       0.50      0.75      0.60         4\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.74      0.84      0.78        45\n",
      "weighted avg       0.93      0.91      0.92        45\n",
      "\n",
      "DT:                precision    recall  f1-score   support\n",
      "\n",
      "           M       0.95      0.95      0.95        41\n",
      "           m       0.50      0.50      0.50         4\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.73      0.73      0.73        45\n",
      "weighted avg       0.91      0.91      0.91        45\n",
      "\n",
      "KP:                precision    recall  f1-score   support\n",
      "\n",
      "           M       0.91      1.00      0.95        41\n",
      "           m       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.46      0.50      0.48        45\n",
      "weighted avg       0.83      0.91      0.87        45\n",
      "\n",
      "*********** Rapport avec déséquilibre / weights = [0.5, 0.5]************\n",
      "Maj:               precision    recall  f1-score   support\n",
      "\n",
      "           M       0.86      0.97      0.91        38\n",
      "           m       0.50      0.14      0.22         7\n",
      "\n",
      "    accuracy                           0.84        45\n",
      "   macro avg       0.68      0.56      0.57        45\n",
      "weighted avg       0.80      0.84      0.81        45\n",
      "\n",
      "NB:                precision    recall  f1-score   support\n",
      "\n",
      "           M       0.88      0.97      0.93        38\n",
      "           m       0.67      0.29      0.40         7\n",
      "\n",
      "    accuracy                           0.87        45\n",
      "   macro avg       0.77      0.63      0.66        45\n",
      "weighted avg       0.85      0.87      0.84        45\n",
      "\n",
      "DT:                precision    recall  f1-score   support\n",
      "\n",
      "           M       0.88      0.97      0.93        38\n",
      "           m       0.67      0.29      0.40         7\n",
      "\n",
      "    accuracy                           0.87        45\n",
      "   macro avg       0.77      0.63      0.66        45\n",
      "weighted avg       0.85      0.87      0.84        45\n",
      "\n",
      "KP:                precision    recall  f1-score   support\n",
      "\n",
      "           M       0.84      1.00      0.92        38\n",
      "           m       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.84        45\n",
      "   macro avg       0.42      0.50      0.46        45\n",
      "weighted avg       0.71      0.84      0.77        45\n",
      "\n",
      "C:\\Users\\Artau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Artau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Artau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Artau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Artau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Artau\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "l_weights=[[0.1, 0.9],[0.2,0.8],[0.3,0.7],[0.5,0.5]]\n",
    "\n",
    "for i in l_weights : \n",
    "    X,Y = make_classification(n_samples=150, n_features=20, n_informative=10,\n",
    "                                n_redundant=5, n_repeated=5, n_classes=2,\n",
    "                                n_clusters_per_class=2, weights=[0.9, 0.1],\n",
    "                                flip_y=0.01, class_sep=1.5, hypercube=True)\n",
    "    cnames=[\"M\",\"m\"]\n",
    "\n",
    "    X_app,X_test,Y_app,Y_test=train_test_split(X,Y,test_size=0.30,random_state=12)\n",
    "\n",
    "    print(\"*********** Rapport avec déséquilibre / weights = {0}************\".format(i)) \n",
    "    \n",
    "    clf_nb = DummyClassifier(strategy=\"stratified\") \n",
    "    clf_nb.fit(X_app,Y_app)\n",
    "    y_pred_nb = clf_nb.predict(X_test)\n",
    "    print(\"Maj:\",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "    \n",
    "    clf_nb = GaussianNB()\n",
    "    clf_nb.fit(X_app,Y_app)\n",
    "    y_pred_nb = clf_nb.predict(X_test)\n",
    "    print(\"NB: \",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "    clf_dt = DecisionTreeClassifier()\n",
    "    clf_dt.fit(X_app,Y_app)\n",
    "    y_pred_nb = clf_dt.predict(X_test)\n",
    "    print(\"DT: \",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "    clf_kppv = KNeighborsClassifier()\n",
    "    clf_kppv.fit(X_app,Y_app)\n",
    "    y_pred_nb = clf_kppv.predict(X_test)\n",
    "    print(\"KP: \",classification_report(Y_test, y_pred_nb, target_names=cnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La diminution du déséquilibre permet d'observer une amélioration des résultats pour certains modèles, tandis que d'autres parraissent meilleurs lorsque le weight est déséquilibré. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Sur de vraies données  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30,random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Rapport avec déséquilibre ************\n",
      "Maj:               precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.32      0.38      0.35        64\n",
      "      benign       0.59      0.53      0.56       107\n",
      "\n",
      "    accuracy                           0.47       171\n",
      "   macro avg       0.46      0.45      0.45       171\n",
      "weighted avg       0.49      0.47      0.48       171\n",
      "\n",
      "NB:                precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.95      0.84      0.89        64\n",
      "      benign       0.91      0.97      0.94       107\n",
      "\n",
      "    accuracy                           0.92       171\n",
      "   macro avg       0.93      0.91      0.92       171\n",
      "weighted avg       0.93      0.92      0.92       171\n",
      "\n",
      "DT:                precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.89      0.84      0.86        64\n",
      "      benign       0.91      0.93      0.92       107\n",
      "\n",
      "    accuracy                           0.90       171\n",
      "   macro avg       0.90      0.89      0.89       171\n",
      "weighted avg       0.90      0.90      0.90       171\n",
      "\n",
      "KP:                precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.91      0.78      0.84        64\n",
      "      benign       0.88      0.95      0.91       107\n",
      "\n",
      "    accuracy                           0.89       171\n",
      "   macro avg       0.89      0.87      0.88       171\n",
      "weighted avg       0.89      0.89      0.89       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnames=list(data.target_names)\n",
    "\n",
    "print(\"*********** Rapport avec déséquilibre ************\") \n",
    "\n",
    "clf_nb = DummyClassifier(strategy=\"stratified\") \n",
    "clf_nb.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_nb.predict(X_test)\n",
    "print(\"Maj:\",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "clf_nb = GaussianNB()\n",
    "clf_nb.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_nb.predict(X_test)\n",
    "print(\"NB: \",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_dt.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_dt.predict(X_test)\n",
    "print(\"DT: \",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "clf_kppv = KNeighborsClassifier()\n",
    "clf_kppv.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_kppv.predict(X_test)\n",
    "print(\"KP: \",classification_report(Y_test, y_pred_nb, target_names=cnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([212, 212])\n"
     ]
    }
   ],
   "source": [
    "while max(Counter(y).values()) - min(Counter(y).values()) != 0:\n",
    "    i = randint(0, len(y))\n",
    "    if y[i] == 1 : \n",
    "        X = np.delete(X,(i),axis=0)\n",
    "        y = np.delete(y,i)\n",
    "print(Counter(y).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424, 30)\n",
      "(424,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30,random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Rapport avec déséquilibre ************\n",
      "Maj:               precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.44      0.49      0.47        61\n",
      "      benign       0.48      0.43      0.46        67\n",
      "\n",
      "    accuracy                           0.46       128\n",
      "   macro avg       0.46      0.46      0.46       128\n",
      "weighted avg       0.46      0.46      0.46       128\n",
      "\n",
      "NB:                precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.93      0.90      0.92        61\n",
      "      benign       0.91      0.94      0.93        67\n",
      "\n",
      "    accuracy                           0.92       128\n",
      "   macro avg       0.92      0.92      0.92       128\n",
      "weighted avg       0.92      0.92      0.92       128\n",
      "\n",
      "DT:                precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.90      0.98      0.94        61\n",
      "      benign       0.98      0.90      0.94        67\n",
      "\n",
      "    accuracy                           0.94       128\n",
      "   macro avg       0.94      0.94      0.94       128\n",
      "weighted avg       0.94      0.94      0.94       128\n",
      "\n",
      "KP:                precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.86      0.93      0.90        61\n",
      "      benign       0.94      0.87      0.90        67\n",
      "\n",
      "    accuracy                           0.90       128\n",
      "   macro avg       0.90      0.90      0.90       128\n",
      "weighted avg       0.90      0.90      0.90       128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"*********** Rapport avec déséquilibre ************\") \n",
    "\n",
    "clf_nb = DummyClassifier(strategy=\"stratified\") \n",
    "clf_nb.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_nb.predict(X_test)\n",
    "print(\"Maj:\",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "clf_nb = GaussianNB()\n",
    "clf_nb.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_nb.predict(X_test)\n",
    "print(\"NB: \",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_dt.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_dt.predict(X_test)\n",
    "print(\"DT: \",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "clf_kppv = KNeighborsClassifier()\n",
    "clf_kppv.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_kppv.predict(X_test)\n",
    "print(\"KP: \",classification_report(Y_test, y_pred_nb, target_names=cnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ère technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([357, 357])\n"
     ]
    }
   ],
   "source": [
    "while max(Counter(y).values()) != min(Counter(y).values()):\n",
    "    i = randint(0, len(y)-1)\n",
    "    if y[i] == 0 :\n",
    "        X = np.append(X,[X[i]], axis = 0)\n",
    "        y = np.append(y,[y[i]], axis = 0)\n",
    "\n",
    "print(Counter(y).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714, 30)\n",
      "(714,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30,random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Rapport avec déséquilibre ************\n",
      "Maj:               precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.62      0.56      0.59       117\n",
      "      benign       0.53      0.59      0.56        98\n",
      "\n",
      "    accuracy                           0.58       215\n",
      "   macro avg       0.58      0.58      0.58       215\n",
      "weighted avg       0.58      0.58      0.58       215\n",
      "\n",
      "NB:                precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.97      0.91      0.94       117\n",
      "      benign       0.90      0.97      0.94        98\n",
      "\n",
      "    accuracy                           0.94       215\n",
      "   macro avg       0.94      0.94      0.94       215\n",
      "weighted avg       0.94      0.94      0.94       215\n",
      "\n",
      "DT:                precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.96      0.91      0.93       117\n",
      "      benign       0.90      0.96      0.93        98\n",
      "\n",
      "    accuracy                           0.93       215\n",
      "   macro avg       0.93      0.93      0.93       215\n",
      "weighted avg       0.93      0.93      0.93       215\n",
      "\n",
      "KP:                precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.98      0.90      0.94       117\n",
      "      benign       0.89      0.98      0.93        98\n",
      "\n",
      "    accuracy                           0.93       215\n",
      "   macro avg       0.94      0.94      0.93       215\n",
      "weighted avg       0.94      0.93      0.94       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"*********** Rapport avec déséquilibre ************\") \n",
    "\n",
    "clf_nb = DummyClassifier(strategy=\"stratified\") \n",
    "clf_nb.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_nb.predict(X_test)\n",
    "print(\"Maj:\",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "clf_nb = GaussianNB()\n",
    "clf_nb.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_nb.predict(X_test)\n",
    "print(\"NB: \",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_dt.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_dt.predict(X_test)\n",
    "print(\"DT: \",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "clf_kppv = KNeighborsClassifier()\n",
    "clf_kppv.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_kppv.predict(X_test)\n",
    "print(\"KP: \",classification_report(Y_test, y_pred_nb, target_names=cnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2ème technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([357, 357])\n"
     ]
    }
   ],
   "source": [
    "while max(Counter(y).values()) != min(Counter(y).values()):\n",
    "    i = randint(0, len(y)-1)\n",
    "    if y[i] == 0 :\n",
    "        mu = abs(np.mean(y[i]))\n",
    "        sigma = np.std(y[i])\n",
    "        noise = np.random.normal(mu, sigma, 30)\n",
    "        noisy = X[i].copy()\n",
    "        noisy += noise\n",
    "        X = np.append(X,[noisy], axis = 0)\n",
    "        y = np.append(y,[y[i]], axis = 0)\n",
    "\n",
    "print(Counter(y).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714, 30)\n",
      "(714,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30,random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Rapport avec déséquilibre ************\n",
      "Maj:               precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.49      0.41      0.45       117\n",
      "      benign       0.41      0.49      0.45        98\n",
      "\n",
      "    accuracy                           0.45       215\n",
      "   macro avg       0.45      0.45      0.45       215\n",
      "weighted avg       0.45      0.45      0.45       215\n",
      "\n",
      "NB:                precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.98      0.86      0.92       117\n",
      "      benign       0.86      0.98      0.91        98\n",
      "\n",
      "    accuracy                           0.92       215\n",
      "   macro avg       0.92      0.92      0.92       215\n",
      "weighted avg       0.92      0.92      0.92       215\n",
      "\n",
      "DT:                precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.92      0.93      0.93       117\n",
      "      benign       0.92      0.91      0.91        98\n",
      "\n",
      "    accuracy                           0.92       215\n",
      "   macro avg       0.92      0.92      0.92       215\n",
      "weighted avg       0.92      0.92      0.92       215\n",
      "\n",
      "KP:                precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.98      0.91      0.94       117\n",
      "      benign       0.90      0.98      0.94        98\n",
      "\n",
      "    accuracy                           0.94       215\n",
      "   macro avg       0.94      0.94      0.94       215\n",
      "weighted avg       0.94      0.94      0.94       215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"*********** Rapport avec déséquilibre ************\") \n",
    "\n",
    "clf_nb = DummyClassifier(strategy=\"stratified\") \n",
    "clf_nb.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_nb.predict(X_test)\n",
    "print(\"Maj:\",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "clf_nb = GaussianNB()\n",
    "clf_nb.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_nb.predict(X_test)\n",
    "print(\"NB: \",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_dt.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_dt.predict(X_test)\n",
    "print(\"DT: \",classification_report(Y_test, y_pred_nb, target_names=cnames))\n",
    "\n",
    "clf_kppv = KNeighborsClassifier()\n",
    "clf_kppv.fit(X_train,Y_train)\n",
    "y_pred_nb = clf_kppv.predict(X_test)\n",
    "print(\"KP: \",classification_report(Y_test, y_pred_nb, target_names=cnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Challenge sur donnees réelles (Kaggle), pour pratiquer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('titanic.csv', sep = ';')\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'] = le.fit_transform(df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "S\n",
      "<ipython-input-52-a6c8d84d76f8>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Embarked'][df['Embarked'].isnull()] = val_remplacement\n"
     ]
    }
   ],
   "source": [
    "nb_manquantes = len(df['Embarked'][df.Embarked.isnull()])\n",
    "val_remplacement = df['Embarked'].dropna().mode().values\n",
    "df['Embarked'][df['Embarked'].isnull()] = val_remplacement\n",
    "\n",
    "print(df.iloc[96,11])\n",
    "print(df.iloc[99,11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux valeurs manquantes ont été remplacées par la valeur S, qui est présente en majorité dans la colonne Embarked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Embarked'] = le.fit_transform(df['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Survived', 'Cabin', 'Name', 'Ticket', 'PassengerId','Fare'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
